{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlNIA7nNedop"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_PDoDOL0AUv",
        "outputId": "6d891e9a-9681-4cd3-ca7e-5950b73ae3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Collecting crcmod\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Building wheels for collected packages: crcmod\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=c4de9b30d916a36f7a029cd3faab0b817c73450c28347f0f19955ccb2d8505f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "Successfully built crcmod\n",
            "Installing collected packages: crcmod\n",
            "Successfully installed crcmod-1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install crcmod wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MqzUkoB0P7Q",
        "outputId": "609ecd21-a63a-4803-f5d8-6b16f1e2b40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://hien7613storage2/kgt5_data.pt...\n",
            "- [1 files][ 87.0 MiB/ 87.0 MiB]                                                \n",
            "Operation completed over 1 objects/87.0 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp  gs://hien7613storage2/kgt5_data.pt /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19rYU13dlNu-"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG3zMb__-IHb"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1zxDoxXtKlB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "wMg3Wtq8IU2x",
        "outputId": "2d2ad317-4348-49d2-d24f-381979fa3946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small', padding=True)\n",
        "\n",
        "def _tokenize( x):\n",
        "    return tokenizer(x, return_tensors=\"pt\")['input_ids'][0][:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcx9-_mCtO-N"
      },
      "source": [
        "## Hop1Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jvn8l5ZPQCs"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Hop1Index:\n",
        "    def __init__(self, triples, num_entities, key_col=0, max_context_size=64):\n",
        "        self.max_context_size = max_context_size\n",
        "        self.shuffle = False\n",
        "        self.key_col = key_col\n",
        "        self.triples = triples[triples[:, key_col].argsort()]\n",
        "        keys, values_offset = np.unique(\n",
        "            self.triples[:, key_col], axis=0, return_index=True\n",
        "        )\n",
        "        values_offset = np.append(values_offset, len(self.triples))\n",
        "        self.keys = keys\n",
        "        self.values_offset = values_offset\n",
        "\n",
        "        self.key_to_start = -1 * np.ones(num_entities, dtype=int)\n",
        "        self.key_to_start[keys] = values_offset[:-1]\n",
        "        self.key_to_end = -1 * np.ones(num_entities, dtype=int)\n",
        "        self.key_to_end[keys] = values_offset[1:]\n",
        "\n",
        "    def __getitem__(self, item, rel_id=None):\n",
        "        start = self.key_to_start[item]\n",
        "        end = self.key_to_end[item]\n",
        "        context = self.triples[start:end, [1, 2 - self.key_col]]\n",
        "        if rel_id is not None:\n",
        "            context = context[context[:,0] == rel_id][:,1]\n",
        "        if len(context) > self.max_context_size:\n",
        "            ids = np.random.choice(len(context), self.max_context_size, replace=False)\n",
        "            context = context[ids]\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(context)\n",
        "        return context\n",
        "\n",
        "    def get_context(self, item, rel_id=None):\n",
        "        return self.__getitem__(item, rel_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p570nXabtSWM"
      },
      "source": [
        "## KGCDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ncnHhD3r0kL",
        "outputId": "aaff5bd7-711d-4ae7-e1e8-24047cf0eaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d1e9fbb2f084>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  kgt5_data = torch.load('/content/kgt5_data.pt')\n"
          ]
        }
      ],
      "source": [
        "kgt5_data = torch.load('/content/kgt5_data.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB_qDcw5jVpJ"
      },
      "outputs": [],
      "source": [
        "from numpy import pi\n",
        "import warnings\n",
        "\n",
        "class RotatE:\n",
        "    def __init__(self, k, max_rel_size=None, entity_embedding=None, relation_embedding=None):\n",
        "        self.internal_k = 2 * k\n",
        "        self.max_rel_size = max_rel_size\n",
        "        self.entity_embedding = entity_embedding\n",
        "        self.relation_embedding = relation_embedding\n",
        "\n",
        "    def __call__(self, e_s_id, e_p_id):\n",
        "        e_s = self.entity_embedding[e_s_id]\n",
        "        e_p = self.relation_embedding[e_p_id]\n",
        "        e_s_real, e_s_img = torch.chunk(e_s, 2, axis=0)\n",
        "        theta_pred, _ = torch.chunk(e_p, 2, axis=0)\n",
        "\n",
        "        embedding_range = (6 / (self.internal_k * self.max_rel_size)) ** 0.5\n",
        "        e_p_real = torch.cos(theta_pred / (embedding_range / pi))\n",
        "        e_p_img = torch.sin(theta_pred / (embedding_range / pi))\n",
        "\n",
        "        e_o_real = e_s_real * e_p_real - e_s_img * e_p_img\n",
        "        e_o_img = e_s_real * e_p_img + e_s_img * e_p_real\n",
        "        return torch.cat([e_o_real, e_o_img], axis=0)\n",
        "\n",
        "rotatE = RotatE(k=350, entity_embedding=kgt5_data['RotatE_ent_emb'], relation_embedding=kgt5_data['RotatE_rel_emb'], max_rel_size=237)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiHkXbP4Vsm3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Dict, Optional, Union, Tuple, List\n",
        "import random\n",
        "\n",
        "\n",
        "class KGCDataset(Dataset):\n",
        "    def __init__(self, num_ents=14541, structal_model=None):\n",
        "        self.num_ents = num_ents\n",
        "        self.structal_model = structal_model\n",
        "        index = 3000\n",
        "        # Fb15k wn18rr\n",
        "        self.id_triplets ={\n",
        "            'train': kgt5_data['train_triplet_id'],\n",
        "            'valid': kgt5_data['valid_triplet_id'],\n",
        "            'test': kgt5_data['test_triplet_id']\n",
        "        }\n",
        "        self.tokens_triplets ={\n",
        "            'train': kgt5_data['train_triplet_tokens'],\n",
        "            'valid': kgt5_data['valid_triplet_tokens'],\n",
        "            'test': kgt5_data['test_triplet_tokens']\n",
        "        }\n",
        "        self.decs_triplets ={\n",
        "            'train': kgt5_data['train_triplet_decs'],\n",
        "            'valid': kgt5_data['valid_triplet_decs'],\n",
        "            'test': kgt5_data['test_triplet_decs']\n",
        "        }\n",
        "\n",
        "        self.get_neigs_0 ={\n",
        "            'train': Hop1Index(self.id_triplets['train'], self.num_ents, 0),\n",
        "            'valid': Hop1Index(\n",
        "                self.id_triplets['valid'],\n",
        "                self.num_ents, 0),\n",
        "            'test': Hop1Index(\n",
        "                kgt5_data['test_triplet_id'],\n",
        "                self.num_ents, 0)\n",
        "        }\n",
        "        self.get_neigs_2 ={\n",
        "            'train': Hop1Index(self.id_triplets['train'], self.num_ents, 2),\n",
        "            'valid': Hop1Index(\n",
        "                self.id_triplets['valid'],\n",
        "                self.num_ents, 2),\n",
        "            'test': Hop1Index(\n",
        "                kgt5_data['test_triplet_id'],\n",
        "                self.num_ents, 2)\n",
        "        }\n",
        "\n",
        "        self.mask_token = _tokenize('<extra_id_90>')\n",
        "        self.eos_token = torch.tensor([tokenizer.eos_token_id])\n",
        "        self.zero_neig_embedding = torch.zeros([512])\n",
        "\n",
        "        self.predict_head_token = _tokenize('predict head :')\n",
        "        self.predict_tail_token = _tokenize('predict tail :')\n",
        "        self.start_decs_token = _tokenize('[')\n",
        "        self.end_decs_token = _tokenize(']')\n",
        "        self.inversion_token = _tokenize('inversion of ')\n",
        "        self.empty_token = torch.tensor([], dtype=torch.int)\n",
        "        self.set_ent_id = set(range(self.num_ents))\n",
        "        self.p_dropout = 0. # 0.2 when training\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.get(idx, split=self.split)\n",
        "    def __len__(self, split='train'):\n",
        "        return len(self.tokens_triplets[split])\n",
        "\n",
        "    def get(self, idx: int, split: str = \"train\", full_mask_part_idx=None):\n",
        "        head_lbl, relation, tail_lbl = self.tokens_triplets[split][idx]\n",
        "        head_id, rel_id, tail_id = self.id_triplets[split][idx]\n",
        "        head_decs, tail_decs = self.decs_triplets[split][idx]\n",
        "\n",
        "        if full_mask_part_idx is None:\n",
        "          full_mask_part_idx = 2 if random.randint(0, 1) else 0\n",
        "\n",
        "        inversion = False\n",
        "\n",
        "        if full_mask_part_idx:\n",
        "          source = [\n",
        "              self.predict_tail_token if not inversion else self.predict_head_token,\n",
        "              head_lbl,\n",
        "              self.start_decs_token,\n",
        "              head_decs,\n",
        "              self.end_decs_token,\n",
        "              self.inversion_token if inversion else self.empty_token,\n",
        "              relation,\n",
        "          ]\n",
        "          target = [tail_lbl]\n",
        "          label_id = tail_id\n",
        "          # filter_id = torch.cat([set_neig.get_context(head_id, rel_id) for set_neig in self.get_neigs_0.values()])\n",
        "          neighboors_0 = self.get_neigs_0[split][head_id]\n",
        "          neighboors_0 = neighboors_0[(neighboors_0[:,0]!=rel_id) | (neighboors_0[:,1]!=tail_id)]\n",
        "          neighboors_2 = self.get_neigs_2[split][head_id]\n",
        "          neighboors_2 = neighboors_2[(neighboors_2[:,0]!=rel_id) | (neighboors_2[:,1]!=tail_id)]\n",
        "        else:\n",
        "          source = [\n",
        "              self.predict_head_token if not inversion else self.predict_tail_token,\n",
        "              tail_lbl,\n",
        "              self.start_decs_token,\n",
        "              tail_decs,\n",
        "              self.end_decs_token,\n",
        "              self.inversion_token if inversion else self.empty_token,\n",
        "              relation,\n",
        "          ]\n",
        "          target = [head_lbl]\n",
        "          label_id = head_id\n",
        "          # filter_id = torch.cat([set_neig.get_context(tail_id, rel_id) for set_neig in self.get_neigs_2.values()])\n",
        "          neighboors_0 = self.get_neigs_0[split][tail_id]\n",
        "          neighboors_0 = neighboors_0[(neighboors_0[:,0]!=rel_id) | (neighboors_0[:,1]!=head_id)]\n",
        "          neighboors_2 = self.get_neigs_2[split][tail_id]\n",
        "          neighboors_2 = neighboors_2[(neighboors_2[:,0]!=rel_id) | (neighboors_2[:,1]!=head_id)]\n",
        "\n",
        "        target_ent_embeddings = []\n",
        "        neighboors_embeddings = []\n",
        "        for rel_n_id, ent_n_id in neighboors_0:\n",
        "          if ent_n_id >= 14505:\n",
        "            continue\n",
        "          ent_n_embedding = self.structal_model.entity_embedding[ent_n_id]\n",
        "          rel_n_embedding = self.structal_model.relation_embedding[rel_n_id]\n",
        "          target_ent_embedding = self.structal_model(ent_n_id, rel_n_id)\n",
        "          neighboors_embeddings.append(torch.cat([ent_n_embedding, rel_n_embedding]))\n",
        "          target_ent_embeddings.append(target_ent_embedding)\n",
        "        for rel_n_id, ent_n_id in neighboors_2:\n",
        "          if ent_n_id >= 14505:\n",
        "            continue\n",
        "          ent_n_embedding = self.structal_model.entity_embedding[ent_n_id]\n",
        "          rel_n_embedding = self.structal_model.relation_embedding[rel_n_id]\n",
        "          target_ent_embedding = self.structal_model(ent_n_id, rel_n_id)\n",
        "          neighboors_embeddings.append(torch.cat([ent_n_embedding, -rel_n_embedding]))\n",
        "          target_ent_embeddings.append(target_ent_embedding)\n",
        "\n",
        "        if len(neighboors_embeddings):\n",
        "          neighboors_embeddings = torch.stack(neighboors_embeddings)\n",
        "          target_ent_embeddings = torch.stack(target_ent_embeddings)\n",
        "          neighboors_embeddings_mask = torch.ones(len(neighboors_embeddings))\n",
        "        else:\n",
        "          neighboors_embeddings_mask = torch.zeros([1])\n",
        "          neighboors_embeddings = torch.zeros([1, 700*2])\n",
        "          target_ent_embeddings = torch.zeros([1, 700])\n",
        "\n",
        "\n",
        "        source.append(self.eos_token)\n",
        "        target.append(self.eos_token)\n",
        "        source = torch.cat(source)\n",
        "        target = torch.cat(target)\n",
        "\n",
        "        attention_mask = torch.ones_like(source)\n",
        "        rand = torch.rand_like(attention_mask.float())\n",
        "        dropout = torch.logical_not(rand < self.p_dropout).long()\n",
        "        dropout[(source == self.start_decs_token[0]) | (source == self.end_decs_token[0])] = 1\n",
        "        dropout[:4]=1\n",
        "        inversion_len = len(self.inversion_token if inversion else self.empty_token)\n",
        "        relation_len = len(relation)\n",
        "        dropout[-relation_len-inversion_len:-relation_len]=1\n",
        "        attention_mask = attention_mask * dropout\n",
        "\n",
        "\n",
        "        output = {\n",
        "            \"input_ids\": source,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": target,\n",
        "            # 'neighboors_embeddings': neighboors_embeddings,\n",
        "            # 'neighboors_embeddings_mask': neighboors_embeddings_mask,\n",
        "            # 'target_ent_embeddings': target_ent_embeddings,\n",
        "            # 'triplet': self.id_triplets[split][idx],\n",
        "        }\n",
        "        return output\n",
        "\n",
        "dataset = KGCDataset(num_ents=14541, structal_model=rotatE)\n",
        "\n",
        "ext_get_neigs_0 ={\n",
        "    'train': Hop1Index(\n",
        "        kgt5_data['train_triplet_id'],\n",
        "        dataset.num_ents, 0, max_context_size=1e10),\n",
        "    'valid': Hop1Index(\n",
        "        kgt5_data['valid_triplet_id'],\n",
        "        dataset.num_ents, 0, max_context_size=1e10),\n",
        "    'test': Hop1Index(\n",
        "        kgt5_data['test_triplet_id'],\n",
        "        dataset.num_ents, 0, max_context_size=1e10),\n",
        "}\n",
        "\n",
        "ext_get_neigs_2 ={\n",
        "    'train': Hop1Index(\n",
        "        kgt5_data['train_triplet_id'],\n",
        "        dataset.num_ents, 2, max_context_size=1e10),\n",
        "    'valid': Hop1Index(\n",
        "        kgt5_data['valid_triplet_id'],\n",
        "        dataset.num_ents, 2, max_context_size=1e10),\n",
        "    'test': Hop1Index(\n",
        "        kgt5_data['test_triplet_id'],\n",
        "        dataset.num_ents, 2, max_context_size=1e10),\n",
        "}\n",
        "\n",
        "# get all ground truth\n",
        "def get_neigs2(ent_id, rel_id):\n",
        "  n_train = ext_get_neigs_2['train'].__getitem__(ent_id, rel_id)\n",
        "  n_valid = ext_get_neigs_2['valid'].__getitem__(ent_id, rel_id)\n",
        "  n_test = ext_get_neigs_2['test'].__getitem__(ent_id, rel_id)\n",
        "  return [n_train, n_valid, n_test]\n",
        "# get all ground truth\n",
        "def get_neigs0(ent_id, rel_id):\n",
        "  n_train = ext_get_neigs_0['train'].__getitem__(ent_id, rel_id)\n",
        "  n_valid = ext_get_neigs_0['valid'].__getitem__(ent_id, rel_id)\n",
        "  n_test = ext_get_neigs_0['test'].__getitem__(ent_id, rel_id)\n",
        "  return [ n_train, n_valid, n_test]\n",
        "\n",
        "class SplitDatasetWrapper:\n",
        "    def __init__(self, dataset, split, full_mask_part_idx=None):\n",
        "        self.dataset = dataset\n",
        "        self.split = split\n",
        "        self.full_mask_part_idx = full_mask_part_idx\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.get(idx, self.split, self.full_mask_part_idx)\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__(split=self.split)\n",
        "\n",
        "train_dataset = SplitDatasetWrapper(dataset, split=\"train\")\n",
        "valid_dataset = SplitDatasetWrapper(dataset, split=\"valid\")\n",
        "test_dataset = SplitDatasetWrapper(dataset, split=\"test\")\n",
        "\n",
        "head_test_dataset = SplitDatasetWrapper(dataset, split=\"test\", full_mask_part_idx=0)\n",
        "tail_test_dataset = SplitDatasetWrapper(dataset, split=\"test\", full_mask_part_idx=2)\n",
        "\n",
        "head_valid_dataset = SplitDatasetWrapper(dataset, split=\"valid\", full_mask_part_idx=0)\n",
        "tail_valid_dataset = SplitDatasetWrapper(dataset, split=\"valid\", full_mask_part_idx=2)\n",
        "\n",
        "head_train_dataset = SplitDatasetWrapper(dataset, split=\"train\", full_mask_part_idx=0)\n",
        "tail_train_dataset = SplitDatasetWrapper(dataset, split=\"train\", full_mask_part_idx=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soSjhyrv0rTj"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYqg4kgb26tO",
        "outputId": "987f5fbd-7f09-4a94-ef66-97bb17072aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model_name='t5-small'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "# model.to('cuda')\n",
        "\n",
        "print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x-heAzKfyhZ"
      },
      "outputs": [],
      "source": [
        "# # !rm /content/kgt5_rotatE_x5.pt\n",
        "# !gsutil cp  gs://hien7613storage2/kgt5_rotatE_x11.pt /content/\n",
        "# state_dict = torch.load('kgt5_rotatE_x11.pt', map_location=torch.device('cpu'))\n",
        "# model.load_state_dict(state_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sryfS0zRtU17"
      },
      "source": [
        "## DataCollatorForSeq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMnzDyFLnEvX"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class DataCollatorForSeq2Seq:\n",
        "    model= None\n",
        "    padding= True\n",
        "    max_length= None\n",
        "    pad_to_multiple_of=None\n",
        "    label_pad_token_id= -100\n",
        "    data_names = None\n",
        "    def __init__(self, tokenizer, model=None, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100,data_names=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.data_names = data_names\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "\n",
        "    def __call__(self, features):\n",
        "        features2 = {}\n",
        "        for name in self.data_names:\n",
        "          if name == 'triplet':\n",
        "            continue\n",
        "          if name in ['labels','filter_id']:\n",
        "            padding_value=self.label_pad_token_id\n",
        "          else:\n",
        "            padding_value=self.tokenizer.pad_token_id\n",
        "          x_features = [feature[name] for feature in features]\n",
        "          features2[name] = torch.nn.utils.rnn.pad_sequence(x_features, batch_first=True, padding_value=padding_value)\n",
        "        if self.model is not None and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\"):\n",
        "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features2[\"labels\"])\n",
        "            features2[\"decoder_input_ids\"] = decoder_input_ids\n",
        "        return features2\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, data_names=list(train_dataset[0].keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUMezYrivbTV",
        "outputId": "1039f2c8-bcf2-4a87-d418-00b4ba00a0a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 9689,   819,     3,    10,  5750,   784,    71, 20237,    19,     3,\n",
              "             9,   607,    13,   789,    16,    84,   579,    19,  1213,    57,\n",
              "             8,   151,    11,  8675,    79, 11924,     6,    11, 12748,    13,\n",
              "           538,    33,     3,     9,     3,     2,   121, 15727,  1052,     2,\n",
              "          1686,  1066,   145, 19852,  6563,    26,     5,    86,   941,   648,\n",
              "             8,  4903,    13,     3,     9, 20237,    19,    92,  5871,  1643,\n",
              "            12,     3,     9,   789,    84, 17981,     7,     3,     9, 28397,\n",
              "             3,   908,  1128,   684,   607,    13,   789,     1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1]),\n",
              " 'labels': tensor([19169,   152,  5750,     1])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OzmyidwvYOa"
      },
      "outputs": [],
      "source": [
        "data = data_collator([train_dataset[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_9mRRy1BZNI"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNExJyfE1jAb",
        "outputId": "c4397bce-cfb5-4c30-e361-9c4f71929df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, TrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "batch_size= 32*4\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    \"kgt5-rotatE\",\n",
        "    dataloader_num_workers=8,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    num_train_epochs=100,\n",
        "    do_eval=True,\n",
        "\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy='epoch',\n",
        "\n",
        "    learning_rate=1e-4,\n",
        "    # torch_compile=True,\n",
        "    fp16=True,\n",
        "\n",
        "    tf32=True,\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmwNji4X1fwN"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/kgt5-rotatE/checkpoint-25512.zip gs://hien7613storage2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnqUp1wKobR-",
        "outputId": "606dac4d-ab6e-42f3-ed21-fa755918bf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/kgt5-rotatE/checkpoint-25512.zip [Content-Type=application/zip]...\n",
            "/ [0 files][    0.0 B/618.6 MiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "- [1 files][618.6 MiB/618.6 MiB]                                                \n",
            "Operation completed over 1 objects/618.6 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(resume_from_checkpoint='/content/kgt5-rotatE/checkpoint-12756')\n",
        "# baaed1dc0ef02b02dff291c8e0cfacf571bff2f9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "collapsed": true,
        "id": "RaZ-xFKHPQaT",
        "outputId": "f85dd411-ad6e-49aa-8121-4ba7f70fb25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3354: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3033: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_rng_state = torch.load(rng_file)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30217' max='212600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 30217/212600 1:05:26 < 11:23:37, 4.45 it/s, Epoch 14.21/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.785700</td>\n",
              "      <td>1.514204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.719100</td>\n",
              "      <td>1.452341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.655400</td>\n",
              "      <td>1.419580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.609600</td>\n",
              "      <td>1.393566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.570200</td>\n",
              "      <td>1.364775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.538500</td>\n",
              "      <td>1.371552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.801800</td>\n",
              "      <td>4.198689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>5.522400</td>\n",
              "      <td>4.242440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4e93e31161fe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/kgt5-rotatE/checkpoint-12756'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# baaed1dc0ef02b02dff291c8e0cfacf571bff2f9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "PqPBuU80uj4V",
        "outputId": "97cee2b3-f1c2-4b7e-e638-072513c94b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26546' max='212600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 26546/212600 1:37:56 < 11:26:31, 4.52 it/s, Epoch 12.49/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.996500</td>\n",
              "      <td>2.332599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.492400</td>\n",
              "      <td>2.038121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.262600</td>\n",
              "      <td>1.857196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.093300</td>\n",
              "      <td>1.739745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.968800</td>\n",
              "      <td>1.647498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.867400</td>\n",
              "      <td>1.564553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.785700</td>\n",
              "      <td>1.514724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.719600</td>\n",
              "      <td>1.453535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.656200</td>\n",
              "      <td>1.420135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.609800</td>\n",
              "      <td>1.395179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.571200</td>\n",
              "      <td>1.366263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.605500</td>\n",
              "      <td>1.651366</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f24323ecb061>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# baaed1dc0ef02b02dff291c8e0cfacf571bff2f9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3579\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mdo_cross_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         )\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "# baaed1dc0ef02b02dff291c8e0cfacf571bff2f9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPX3KDBPs3z-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "93b2a7fe-c960-446c-92ef-bcde7306aab4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12772' max='212600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 12772/212600 00:03 < 12:50:50, 4.32 it/s, Epoch 6.01/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.867400</td>\n",
              "      <td>1.574712</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5747116804122925}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "trainer.evaluate()\n",
        "# baaed1dc0ef02b02dff291c8e0cfacf571bff2f9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzEndGNJE2gK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "trainer.model.eval()\n",
        "model_state_dict = trainer.model.state_dict()\n",
        "torch.save(model_state_dict, '/content/kgt5_rotatE_x12.pt')\n",
        "\n",
        "!gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp  /content/kgt5_rotatE_x12.pt gs://hien7613storage2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei0VsUL3z2g_"
      },
      "source": [
        "# New Eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMzkCXtKvDQ9"
      },
      "source": [
        "## setup eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701,
          "referenced_widgets": [
            "3a48be1a658540728e35f50a57cdcb30",
            "4074bf3192c2487daadacb8eaaaeba94",
            "15f40e014b1642938808bd6de20d37d4",
            "8a7ba6ca0489415eabb238e7a4b4fec8"
          ]
        },
        "id": "oMkUb24Ns711",
        "outputId": "e4775ad3-092b-4675-fa84-936758f6e653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-25 11:35:46--  https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entity2text.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entity2text.txt [following]\n",
            "--2024-04-25 11:35:46--  https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entity2text.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 392383 (383K) [text/plain]\n",
            "Saving to: ‘entity2text.txt’\n",
            "\n",
            "entity2text.txt     100%[===================>] 383.19K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-04-25 11:35:47 (78.3 MB/s) - ‘entity2text.txt’ saved [392383/392383]\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a48be1a658540728e35f50a57cdcb30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/14951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-25 11:35:49--  https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entities.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entities.txt [following]\n",
            "--2024-04-25 11:35:49--  https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entities.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 142414 (139K) [text/plain]\n",
            "Saving to: ‘entities.txt’\n",
            "\n",
            "entities.txt        100%[===================>] 139.08K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-04-25 11:35:49 (60.7 MB/s) - ‘entities.txt’ saved [142414/142414]\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4074bf3192c2487daadacb8eaaaeba94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/14541 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15f40e014b1642938808bd6de20d37d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/14541 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a7ba6ca0489415eabb238e7a4b4fec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/14541 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ent2text\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/entity2text.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entity2text.txt\n",
        "path = \"/content/entity2text.txt\"\n",
        "\n",
        "ent2text = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    ent, text = line.strip().split('\\t')\n",
        "    ent2text[ent] = _tokenize(text)\n",
        "\n",
        "# ent2id\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/entities.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entities.txt\n",
        "path = \"/content/entities.txt\"\n",
        "\n",
        "ent2id = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    # print(line.strip().split('\\t'))\n",
        "    ent = line.strip().split('\\t')[0]\n",
        "    ent2id[ent] = int(i)\n",
        "\n",
        "entid2text = [0]*len(ent2id)\n",
        "for ent in tqdm(ent2id):\n",
        "  entid2text[ent2id[ent]] = [0] + ent2text[ent].tolist() + [1]\n",
        "\n",
        "ent_name_decode_list = []\n",
        "for target in tqdm(entid2text):\n",
        "  ent_name_decode_list.append(tokenizer.decode(target[1:-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL8jJYBHqUhl"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "class Trie(object):\n",
        "    def __init__(self, sequences: List[List[int]] = []):\n",
        "        self.trie_dict = {}\n",
        "        self.len = 0\n",
        "        if sequences:\n",
        "            for sequence in sequences:\n",
        "                Trie._add_to_trie(sequence, self.trie_dict)\n",
        "                self.len += 1\n",
        "        self.append_trie = None\n",
        "        self.bos_token_id = None\n",
        "    def append(self, trie, bos_token_id):\n",
        "        self.append_trie = trie\n",
        "        self.bos_token_id = bos_token_id\n",
        "    def add(self, sequence: List[int]):\n",
        "        Trie._add_to_trie(sequence, self.trie_dict)\n",
        "        self.len += 1\n",
        "    def get(self, prefix_sequence: List[int]):\n",
        "        return Trie._get_from_trie(prefix_sequence, self.trie_dict, self.append_trie, self.bos_token_id)\n",
        "    @staticmethod\n",
        "    def load_from_dict(trie_dict):\n",
        "        trie = Trie()\n",
        "        trie.trie_dict = trie_dict\n",
        "        trie.len = sum(1 for _ in trie)\n",
        "        return trie\n",
        "    @staticmethod\n",
        "    def _add_to_trie(sequence: List[int], trie_dict: Dict):\n",
        "        if sequence:\n",
        "            if sequence[0] not in trie_dict:\n",
        "                trie_dict[sequence[0]] = {}\n",
        "            Trie._add_to_trie(sequence[1:], trie_dict[sequence[0]])\n",
        "    @staticmethod\n",
        "    def _get_from_trie(\n",
        "        prefix_sequence: List[int],\n",
        "        trie_dict: Dict,\n",
        "        append_trie=None,\n",
        "        bos_token_id: int = None,\n",
        "    ):\n",
        "        if len(prefix_sequence) == 0:\n",
        "            output = list(trie_dict.keys())\n",
        "            if append_trie and bos_token_id in output:\n",
        "                output.remove(bos_token_id)\n",
        "                output += list(append_trie.trie_dict.keys())\n",
        "            if len(output) == 0:\n",
        "                return [0]\n",
        "            return output\n",
        "        elif prefix_sequence[0] in trie_dict:\n",
        "            return Trie._get_from_trie(\n",
        "                prefix_sequence[1:],\n",
        "                trie_dict[prefix_sequence[0]],\n",
        "                append_trie,\n",
        "                bos_token_id,\n",
        "            )\n",
        "        else:\n",
        "            if append_trie:\n",
        "                return append_trie.get(prefix_sequence)\n",
        "            else:\n",
        "                return [0]\n",
        "    def __iter__(self):\n",
        "        def _traverse(prefix_sequence, trie_dict):\n",
        "            if trie_dict:\n",
        "                for next_token in trie_dict:\n",
        "                    yield from _traverse(prefix_sequence + [next_token], trie_dict[next_token])\n",
        "            else:\n",
        "                yield prefix_sequence\n",
        "\n",
        "        return _traverse([], self.trie_dict)\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    def __getitem__(self, value):\n",
        "        return self.get(value)\n",
        "trie = Trie(entid2text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYyxQXRWJKuu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.allow_tf32 = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-rKh9utGqB2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "# from helper import get_performance\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class RunEval:\n",
        "    def __init__(self, configs, model, tokenizer, ent_name_list, target_embeddings):\n",
        "        self.configs = configs\n",
        "        self.ent_name_list = ent_name_list\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.configs = configs\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_step(self, batched_data, dataset_idx):\n",
        "        input_ids = batched_data['input_ids'].to('cuda')\n",
        "        attention_mask = batched_data['attention_mask'].to('cuda')\n",
        "        labels = batched_data['labels']\n",
        "        labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        neighboors_embeddings=batched_data['neighboors_embeddings'].to('cuda')\n",
        "        neighboors_embeddings_mask=batched_data['neighboors_embeddings_mask'].to('cuda')\n",
        "        target_ent_embeddings=batched_data['target_ent_embeddings'].to('cuda')\n",
        "        triple_id = batched_data['triplet'].numpy()\n",
        "\n",
        "        self.get_neigs = get_neigs2 if dataset_idx == 0 else get_neigs0\n",
        "\n",
        "        old_seqs = []\n",
        "        ranks = torch.randint(self.configs.num_beams + 1, self.configs.n_ent, (len(labels),))\n",
        "        for i in range(self.configs.num_beams):\n",
        "          outputs = self.model.generate(\n",
        "              input_ids=input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              return_dict_in_generate=True,\n",
        "              max_length=512,\n",
        "              # eos_token_id=1,\n",
        "              prefix_allowed_tokens_fn=lambda batch_idx, m_input_ids: self._next_candidate(batch_idx, m_input_ids, triple_id, dataset_idx, old_seqs),\n",
        "              # bos_token_id=0,\n",
        "              neighboors_embeddings=neighboors_embeddings,\n",
        "              neighboors_embeddings_mask=neighboors_embeddings_mask,\n",
        "              target_ent_embeddings=target_ent_embeddings,\n",
        "          )\n",
        "          pred = outputs.sequences.cpu()\n",
        "          old_seqs.append(pred)\n",
        "          pred = pred[:,1:]\n",
        "          if pred.shape[1] > labels.shape[1]:\n",
        "            pred = pred[:,:labels.shape[1]]\n",
        "          else:\n",
        "            cut_labels = labels[:,:pred.shape[1]]\n",
        "          cut_labels = labels\n",
        "          seq_match = (pred==cut_labels).all(1)\n",
        "          new_ranks = torch.where(~seq_match, ranks, i+1)\n",
        "          ranks = torch.min(ranks, new_ranks)\n",
        "        # print(labels)\n",
        "        # print(outputs.sequences.cpu()[1])\n",
        "        # print(tokenizer.batch_decode(outputs.sequences.cpu(), skip_special_tokens=True))\n",
        "        # print(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
        "        # raise 'sdfa'\n",
        "\n",
        "        ranks = ranks.tolist()\n",
        "        out = {'ranks': ranks}\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _next_candidate(self, batch_idx, input_ids, triple_id, dataset_idx, old_seqs=None):\n",
        "        input_ids = input_ids.cpu()\n",
        "        # print(input_ids.tolist())\n",
        "        # next = trie.get(input_ids.tolist())\n",
        "        # print(next)\n",
        "        # print('='*30)\n",
        "        # return next\n",
        "        if input_ids[-1] == 0 and len(input_ids) != 1:\n",
        "            return [0]\n",
        "        pred_ids = self.target_embeddings[triple_id[batch_idx][dataset_idx]]\n",
        "        pred_id = int(pred_ids[len(input_ids)])\n",
        "        all_gt_ids = torch.cat(self.get_neigs(triple_id[batch_idx][2-dataset_idx], triple_id[batch_idx][1]))\n",
        "\n",
        "        all_gt_seq = torch.index_select(self.target_embeddings, 0, all_gt_ids)\n",
        "        all_gt_seq_mask = (all_gt_seq[:, :len(input_ids)]==input_ids).all(1)\n",
        "        all_gt_seq_tokens = all_gt_seq[:, len(input_ids)][all_gt_seq_mask]\n",
        "        if len(old_seqs) > 0:\n",
        "          old_seq = torch.nn.utils.rnn.pad_sequence([x[batch_idx] for x in old_seqs], batch_first=True, padding_value=0)\n",
        "          if old_seq.shape[1] > len(input_ids):\n",
        "            old_seq_mask = (old_seq[:, :len(input_ids)]==input_ids).all(1)\n",
        "            old_seq_tokens = old_seq[:, len(input_ids)][old_seq_mask]\n",
        "          else:\n",
        "            old_seq_tokens = torch.tensor([], dtype=torch.int64)\n",
        "        else:\n",
        "          old_seq_tokens = torch.tensor([], dtype=torch.int64)\n",
        "        all_gt_seq_tokens = set(torch.cat([all_gt_seq_tokens, old_seq_tokens]).tolist())\n",
        "        pred_id = int(pred_ids[len(input_ids)])\n",
        "        next_tokens = set(trie.get(input_ids.tolist())).difference(all_gt_seq_tokens)\n",
        "        if pred_id in all_gt_seq_tokens:\n",
        "          next_tokens.add(pred_id)\n",
        "        if len(next_tokens) == 0:\n",
        "          return [0]\n",
        "        return list(next_tokens)\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        pred_tail_out, pred_head_out = outs\n",
        "        agg_tail_out, agg_head_out, agg_total_out = dict(), dict(), dict()\n",
        "        for out in pred_tail_out:\n",
        "            for key, value in out.items():\n",
        "                if key in agg_tail_out:\n",
        "                    agg_tail_out[key] += value\n",
        "                else:\n",
        "                    agg_tail_out[key] = value\n",
        "        for out in pred_head_out:\n",
        "            for key, value in out.items():\n",
        "                if key in agg_head_out:\n",
        "                    agg_head_out[key] += value\n",
        "                else:\n",
        "                    agg_head_out[key] = value\n",
        "        tail_ranks, head_ranks = agg_tail_out['ranks'], agg_head_out['ranks']\n",
        "        del agg_tail_out['ranks']\n",
        "        del agg_head_out['ranks']\n",
        "        perf = get_performance(self, head_ranks, tail_ranks)\n",
        "        print(perf)\n",
        "        return perf\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GC2QTOMODd4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def _get_performance(ranks):\n",
        "    ranks = np.array(ranks, dtype=np.float32)\n",
        "    out = dict()\n",
        "    out['mr'] = ranks.mean(axis=0)\n",
        "    out['mrr'] = (1. / ranks).mean(axis=0)\n",
        "    out['hit1'] = np.sum(ranks == 1, axis=0) / len(ranks)\n",
        "    out['hit3'] = np.sum(ranks <= 3, axis=0) / len(ranks)\n",
        "    out['hit10'] = np.sum(ranks <= 10, axis=0) / len(ranks)\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_performance(model, tail_ranks, head_ranks):\n",
        "    tail_out = _get_performance(tail_ranks)\n",
        "    head_out = _get_performance(head_ranks)\n",
        "    mr = np.array([tail_out['mr'], head_out['mr']])\n",
        "    mrr = np.array([tail_out['mrr'], head_out['mrr']])\n",
        "    hit1 = np.array([tail_out['hit1'], head_out['hit1']])\n",
        "    hit3 = np.array([tail_out['hit3'], head_out['hit3']])\n",
        "    hit10 = np.array([tail_out['hit10'], head_out['hit10']])\n",
        "    val_mrr = mrr.mean().item()\n",
        "    perf = {'mrr': mrr, 'mr': mr, 'hit@1': hit1, 'hit@3': hit3, 'hit@10': hit10}\n",
        "    perf = pd.DataFrame(perf, index=['tail ranking', 'head ranking'])\n",
        "    perf.loc['mean ranking'] = perf.mean(axis=0)\n",
        "    for hit in ['hit@1', 'hit@3', 'hit@5', 'hit@10']:\n",
        "        if hit in list(perf.columns):\n",
        "            perf[hit] = perf[hit].apply(lambda x: '%.2f%%' % (x * 100))\n",
        "    return perf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVxdc47Tz_n0"
      },
      "outputs": [],
      "source": [
        "entid2text_emb = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in entid2text], batch_first=True, padding_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t1eVInp0NST",
        "outputId": "e0ece641-8320-4dee-8f8f-1cad208e217e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 6458, 9, 1]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "entid2text[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mu34tUZs7K5i",
        "outputId": "47d05547-6718-4848-d149-9ab1df29e1f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'['"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(784)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MigG19X0zwS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "# from helper import get_performance\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class RunEval:\n",
        "    def __init__(self, configs, model, tokenizer, ent_name_list, target_embeddings):\n",
        "        self.configs = configs\n",
        "        self.ent_name_list = ent_name_list\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.configs = configs\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_step(self, batched_data, dataset_idx):\n",
        "        input_ids = batched_data['input_ids'].to('cuda')\n",
        "        attention_mask = batched_data['attention_mask'].to('cuda')\n",
        "        labels = batched_data['labels']\n",
        "        labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        neighboors_embeddings=batched_data['neighboors_embeddings'].to('cuda')\n",
        "        neighboors_embeddings_mask=batched_data['neighboors_embeddings_mask'].to('cuda')\n",
        "        target_ent_embeddings=batched_data['target_ent_embeddings'].to('cuda')\n",
        "        triple_id = batched_data['triplet'].numpy()\n",
        "\n",
        "        self.get_neigs = get_neigs2 if dataset_idx == 0 else get_neigs0\n",
        "\n",
        "        old_seqs = []\n",
        "        ranks = torch.randint(self.configs.num_beams + 1, self.configs.n_ent, (len(labels),))\n",
        "        for i in range(self.configs.num_beams):\n",
        "          outputs = self.model.generate(\n",
        "              input_ids=input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              return_dict_in_generate=True,\n",
        "              max_length=self.configs.max_length,\n",
        "              # eos_token_id=1,\n",
        "              prefix_allowed_tokens_fn=lambda batch_idx, m_input_ids: self._next_candidate(batch_idx, m_input_ids, triple_id, dataset_idx, old_seqs),\n",
        "              # bos_token_id=0,\n",
        "              neighboors_embeddings=neighboors_embeddings,\n",
        "              neighboors_embeddings_mask=neighboors_embeddings_mask,\n",
        "              target_ent_embeddings=target_ent_embeddings,\n",
        "          )\n",
        "          pred = outputs.sequences.cpu()\n",
        "          old_seqs.append(pred)\n",
        "          pred = pred[:,1:]\n",
        "          if pred.shape[1] > labels.shape[1]:\n",
        "            pred = pred[:,:labels.shape[1]]\n",
        "            cut_labels = labels\n",
        "          else:\n",
        "            cut_labels = labels[:,:pred.shape[1]]\n",
        "          seq_match = (pred==cut_labels).all(1)\n",
        "          new_ranks = torch.where(~seq_match, ranks, i+1)\n",
        "          ranks = torch.min(ranks, new_ranks)\n",
        "        # print(labels)\n",
        "        # print(outputs.sequences.cpu()[1])\n",
        "        # print(tokenizer.batch_decode(outputs.sequences.cpu(), skip_special_tokens=True))\n",
        "        # print(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
        "        # raise 'sdfa'\n",
        "\n",
        "        ranks = ranks.tolist()\n",
        "        out = {'ranks': ranks}\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _next_candidate(self, batch_idx, input_ids, triple_id, dataset_idx, old_seqs=None):\n",
        "        input_ids = input_ids.cpu()\n",
        "        # print(input_ids.tolist())\n",
        "        # next = trie.get(input_ids.tolist())\n",
        "        # print(next)\n",
        "        # print('='*30)\n",
        "        # return next\n",
        "        if input_ids[-1] == 0 and len(input_ids) != 1:\n",
        "            return [0]\n",
        "        pred_ids = self.target_embeddings[triple_id[batch_idx][dataset_idx]]\n",
        "        pred_id = int(pred_ids[len(input_ids)])\n",
        "        all_gt_ids = torch.cat(self.get_neigs(triple_id[batch_idx][2-dataset_idx], triple_id[batch_idx][1]))\n",
        "\n",
        "        all_gt_seq = torch.index_select(self.target_embeddings, 0, all_gt_ids)\n",
        "        all_gt_seq_mask = (all_gt_seq[:, :len(input_ids)]==input_ids).all(1)\n",
        "        all_gt_seq_tokens = all_gt_seq[:, len(input_ids)][all_gt_seq_mask]\n",
        "        if len(old_seqs) > 0:\n",
        "          old_seq = torch.nn.utils.rnn.pad_sequence([x[batch_idx] for x in old_seqs], batch_first=True, padding_value=0)\n",
        "          if old_seq.shape[1] > len(input_ids):\n",
        "            old_seq_mask = (old_seq[:, :len(input_ids)]==input_ids).all(1)\n",
        "            old_seq_tokens = old_seq[:, len(input_ids)][old_seq_mask]\n",
        "          else:\n",
        "            old_seq_tokens = torch.tensor([], dtype=torch.int64)\n",
        "        else:\n",
        "          old_seq_tokens = torch.tensor([], dtype=torch.int64)\n",
        "        all_gt_seq_tokens = set(torch.cat([all_gt_seq_tokens, old_seq_tokens]).tolist())\n",
        "        pred_id = int(pred_ids[len(input_ids)])\n",
        "        next_tokens = set(trie.get(input_ids.tolist())).difference(all_gt_seq_tokens)\n",
        "        if pred_id in all_gt_seq_tokens:\n",
        "          next_tokens.add(pred_id)\n",
        "        if len(next_tokens) == 0:\n",
        "          return [0]\n",
        "        return list(next_tokens)\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        pred_tail_out, pred_head_out = outs\n",
        "        agg_tail_out, agg_head_out, agg_total_out = dict(), dict(), dict()\n",
        "        for out in pred_tail_out:\n",
        "            for key, value in out.items():\n",
        "                if key in agg_tail_out:\n",
        "                    agg_tail_out[key] += value\n",
        "                else:\n",
        "                    agg_tail_out[key] = value\n",
        "        for out in pred_head_out:\n",
        "            for key, value in out.items():\n",
        "                if key in agg_head_out:\n",
        "                    agg_head_out[key] += value\n",
        "                else:\n",
        "                    agg_head_out[key] = value\n",
        "        tail_ranks, head_ranks = agg_tail_out['ranks'], agg_head_out['ranks']\n",
        "        del agg_tail_out['ranks']\n",
        "        del agg_head_out['ranks']\n",
        "        perf = get_performance(self, head_ranks, tail_ranks)\n",
        "        print(perf)\n",
        "        return perf\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i7u2pvMu9ba"
      },
      "source": [
        "## run eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1rH5ECR64ap"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class DataCollatorForSeq2Seq:\n",
        "    model= None\n",
        "    padding= True\n",
        "    max_length= None\n",
        "    pad_to_multiple_of=None\n",
        "    label_pad_token_id= -100\n",
        "    data_names = None\n",
        "    def __init__(self, tokenizer, model=None, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100,data_names=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.data_names = data_names\n",
        "        self.label_pad_token_id = label_pad_token_id\n",
        "\n",
        "    def __call__(self, features):\n",
        "        features2 = {}\n",
        "        for name in self.data_names:\n",
        "          # if name == 'triplet':\n",
        "          #   continue\n",
        "          if name in ['labels','filter_id']:\n",
        "            padding_value=self.label_pad_token_id\n",
        "          else:\n",
        "            padding_value=self.tokenizer.pad_token_id\n",
        "          x_features = [feature[name] for feature in features]\n",
        "          features2[name] = torch.nn.utils.rnn.pad_sequence(x_features, batch_first=True, padding_value=padding_value)\n",
        "        if self.model is not None and hasattr(self.model, \"prepare_decoder_input_ids_from_labels\"):\n",
        "            decoder_input_ids = self.model.prepare_decoder_input_ids_from_labels(labels=features2[\"labels\"])\n",
        "            features2[\"decoder_input_ids\"] = decoder_input_ids\n",
        "        return features2\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, data_names=list(train_dataset[0].keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "c2c6223f785e42048709b311d47edf06",
            "c477a55916b64a2ab607b28a7bece6af"
          ]
        },
        "id": "7TdnlnP0FZMq",
        "outputId": "2e77a43b-140f-4781-9b36-88445e99f65f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2c6223f785e42048709b311d47edf06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/320 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c477a55916b64a2ab607b28a7bece6af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/320 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   mrr           mr   hit@1   hit@3  hit@10\n",
            "tail ranking  0.479574  2367.001953  39.12%  53.62%  67.29%\n",
            "head ranking  0.353449  3125.134521  26.15%  40.58%  57.04%\n",
            "mean ranking  0.416512  2746.068359  32.63%  47.10%  62.16%\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tail_data_loader = DataLoader(tail_test_dataset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=data_collator,\n",
        "                          # num_workers=8,\n",
        "                          pin_memory=True)\n",
        "head_data_loader = DataLoader(head_test_dataset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          # num_workers=8,\n",
        "                          collate_fn=data_collator,\n",
        "                          pin_memory=True)\n",
        "\n",
        "\n",
        "class Configs:\n",
        "    def __init__(self):\n",
        "        self.num_beams = 1\n",
        "        # self.num_beam_groups = 1\n",
        "        self.num_return_sequences = 1\n",
        "        self.max_length = 30\n",
        "        self.n_ent = 14541\n",
        "        self.n_rel = 237\n",
        "\n",
        "configs = Configs()\n",
        "runEval = RunEval(configs, model, tokenizer, ent_name_decode_list, entid2text_emb)\n",
        "\n",
        "runEval.model.eval()\n",
        "head_list_result = []\n",
        "for data in tqdm(head_data_loader):\n",
        "    # pass\n",
        "    rank_rs = runEval.validation_step(data, 0)\n",
        "    head_list_result.append(rank_rs)\n",
        "    # break\n",
        "tail_list_result = []\n",
        "for data in tqdm(tail_data_loader):\n",
        "    rank_rs = runEval.validation_step(data, 2)\n",
        "    tail_list_result.append(rank_rs)\n",
        "    # break\n",
        "\n",
        "kq = runEval.validation_epoch_end((head_list_result, tail_list_result))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irdmVEIn47cl"
      },
      "source": [
        "# EditedT5\n",
        "<!--  -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVLOugts4-nb"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import T5Config\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutput\n",
        "\n",
        "\n",
        "def shape(states, batch_size):\n",
        "    return states.view(batch_size, -1, 8, 64).transpose(1, 2)\n",
        "\n",
        "\n",
        "class EditedT5(T5ForConditionalGeneration):\n",
        "  def __init__(self, config: T5Config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.key_projection = nn.Linear(700, config.d_model)\n",
        "    self.value_projection1 = nn.Linear(700*2, 700*4)\n",
        "    self.value_projection2 = nn.Linear(700*4, config.d_model)\n",
        "    self.act = nn.PReLU()\n",
        "\n",
        "    self.post_init()\n",
        "\n",
        "  def k_projection(self, x, i, batch_size):\n",
        "    return shape(self.decoder.block[i].layer[1].EncDecAttention.k(x), batch_size)\n",
        "\n",
        "  def v_projection(self, x, i, batch_size):\n",
        "    return shape(self.decoder.block[i].layer[1].EncDecAttention.v(x), batch_size)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids: Optional[torch.LongTensor] = None,\n",
        "      attention_mask: Optional[torch.FloatTensor] = None,\n",
        "      decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "      decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "      head_mask: Optional[torch.FloatTensor] = None,\n",
        "      decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
        "      cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "      encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "      past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "      inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "      decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "      labels: Optional[torch.LongTensor] = None,\n",
        "      use_cache: Optional[bool] = None,\n",
        "      output_attentions: Optional[bool] = None,\n",
        "      output_hidden_states: Optional[bool] = None,\n",
        "      return_dict: Optional[bool] = None,\n",
        "      target_ent_embeddings=None,\n",
        "      neighboors_embeddings=None,\n",
        "      neighboors_embeddings_mask=None,\n",
        "  ) :\n",
        "\n",
        "      value_embeddings = self.value_projection2(self.act(self.value_projection1(neighboors_embeddings)))\n",
        "      key_embeddings = self.key_projection(target_ent_embeddings) * neighboors_embeddings_mask.unsqueeze(2)\n",
        "      batch_size = value_embeddings.shape[0]\n",
        "\n",
        "      use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "      return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "      if encoder_outputs is None:\n",
        "        # Convert encoder inputs in embeddings if needed\n",
        "        encoder_outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "      elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "          encoder_outputs = BaseModelOutput(\n",
        "              last_hidden_state=encoder_outputs[0],\n",
        "              hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "              attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "          )\n",
        "      hidden_states = encoder_outputs[0]\n",
        "\n",
        "\n",
        "      if past_key_values is None:\n",
        "\n",
        "        past_key_values = []\n",
        "        for i in range(self.config.num_layers):\n",
        "          cross_key = self.k_projection(key_embeddings, i, batch_size)\n",
        "          cross_value = self.v_projection(value_embeddings, i, batch_size)\n",
        "          self_key = torch.zeros_like(cross_key)\n",
        "          self_value = torch.zeros_like(cross_value)\n",
        "          past_key_values.append((self_key, self_value, cross_key, cross_value))\n",
        "\n",
        "\n",
        "      if self.model_parallel:\n",
        "          torch.cuda.set_device(self.decoder.first_device)\n",
        "\n",
        "      if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "          decoder_input_ids = self._shift_right(labels)\n",
        "\n",
        "      if self.model_parallel:\n",
        "          torch.cuda.set_device(self.decoder.first_device)\n",
        "          hidden_states = hidden_states.to(self.decoder.first_device)\n",
        "          if decoder_input_ids is not None:\n",
        "              decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
        "          if attention_mask is not None:\n",
        "              attention_mask = attention_mask.to(self.decoder.first_device)\n",
        "          if decoder_attention_mask is not None:\n",
        "              decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
        "\n",
        "      # Decode\n",
        "      decoder_outputs = self.decoder(\n",
        "          input_ids=decoder_input_ids,\n",
        "          attention_mask=decoder_attention_mask,\n",
        "          inputs_embeds=decoder_inputs_embeds,\n",
        "          past_key_values=past_key_values,\n",
        "          encoder_hidden_states=hidden_states,\n",
        "          encoder_attention_mask=attention_mask,\n",
        "          head_mask=decoder_head_mask,\n",
        "          cross_attn_head_mask=cross_attn_head_mask,\n",
        "          use_cache=use_cache,\n",
        "          output_attentions=output_attentions,\n",
        "          output_hidden_states=output_hidden_states,\n",
        "          return_dict=return_dict,\n",
        "      )\n",
        "\n",
        "      sequence_output = decoder_outputs[0]\n",
        "\n",
        "      # Set device for model parallelism\n",
        "      if self.model_parallel:\n",
        "          torch.cuda.set_device(self.encoder.first_device)\n",
        "          self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
        "          sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
        "\n",
        "      if self.config.tie_word_embeddings:\n",
        "          sequence_output = sequence_output * (self.model_dim**-0.5)\n",
        "\n",
        "      lm_logits = self.lm_head(sequence_output)\n",
        "\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "          loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "          # move labels to correct device to enable PP\n",
        "          labels = labels.to(lm_logits.device)\n",
        "          loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "      if not return_dict:\n",
        "          output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
        "          return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "      return Seq2SeqLMOutput(\n",
        "          loss=loss,\n",
        "          logits=lm_logits,\n",
        "          past_key_values=decoder_outputs.past_key_values,\n",
        "          decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "          decoder_attentions=decoder_outputs.attentions,\n",
        "          cross_attentions=decoder_outputs.cross_attentions,\n",
        "          encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "          encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "          encoder_attentions=encoder_outputs.attentions,\n",
        "      )\n",
        "\n",
        "  def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        input_ids,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        decoder_attention_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        target_ent_embeddings=None,\n",
        "        neighboors_embeddings=None,\n",
        "        neighboors_embeddings_mask=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # cut decoder_input_ids if past_key_values is used\n",
        "        if past_key_values is not None:\n",
        "            past_length = past_key_values[0][0].shape[2]\n",
        "\n",
        "            # Some generation methods already pass only the last input ID\n",
        "            if input_ids.shape[1] > past_length:\n",
        "                remove_prefix_length = past_length\n",
        "            else:\n",
        "                # Default to old behavior: keep only final ID\n",
        "                remove_prefix_length = input_ids.shape[1] - 1\n",
        "\n",
        "            input_ids = input_ids[:, remove_prefix_length:]\n",
        "\n",
        "        return {\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"decoder_attention_mask\": decoder_attention_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,\n",
        "            \"target_ent_embeddings\": target_ent_embeddings,\n",
        "            \"neighboors_embeddings\": neighboors_embeddings,\n",
        "            \"neighboors_embeddings_mask\": neighboors_embeddings_mask,\n",
        "            # \"input_ids\": input_ids,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1lbFKL9RKPD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_bWKKFrjyq"
      },
      "source": [
        "# write dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKp41YiXMOHT",
        "outputId": "85c04a2a-9026-4288-b65a-caca155eea25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model does not include a db file. Skipping.\n"
          ]
        }
      ],
      "source": [
        "from ampligraph.datasets import load_fb15k_237\n",
        "from ampligraph.pretrained_models import load_pretrained_model\n",
        "\n",
        "model = load_pretrained_model(dataset=\"fb15k-237\", scoring_type=\"RotatE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "14f6c6aa8c474cdcaa1bf4d2fc65c3c4"
          ]
        },
        "id": "p6vLmdfZSy5J",
        "outputId": "f4ef0756-8ae5-41e1-e731-408c26a13c2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14f6c6aa8c474cdcaa1bf4d2fc65c3c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/14541 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ent2id\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/entities.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entities.txt\n",
        "path = \"/content/entities.txt\"\n",
        "\n",
        "ent2id = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    # print(line.strip().split('\\t'))\n",
        "    ent = line.strip().split('\\t')[0]\n",
        "    ent2id[ent] = int(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e4f44a1b47d942ea960714e7259d2b1d"
          ]
        },
        "id": "2mjnTf_oU5nV",
        "outputId": "d5623ebf-d865-471a-ada4-9a712bcee7be"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4f44a1b47d942ea960714e7259d2b1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/237 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# rel2id\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/relations.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/relations.txt\n",
        "path = \"/content/relations.txt\"\n",
        "\n",
        "rel2id = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    # print(line.strip().split('\\t'))\n",
        "    rel = line.strip().split('\\t')[0]\n",
        "    rel2id[rel] = int(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPBBeXhXa3TA",
        "outputId": "6aa5c8bd-f310-4e12-ae2c-432effc7d1ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small', padding=True)\n",
        "\n",
        "def _tokenize( x):\n",
        "    return tokenizer(x, return_tensors=\"pt\")['input_ids'][0][:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "f0b409dd6ab344c6837ac8b6436be433",
            "62a21e3f23164eae9d0a222807ccce84",
            "ae5aadc209da4ef8b21e176826290e93",
            "54f2a934e5cf453c8221d684ee8a6e5a",
            "6f82b483b786446a96b7f536ade3fbe9",
            "7f5196b8a08748a698b3cb628bada525",
            "9920b81630794f33a7fec9862e53d477",
            "06ea347494ca48b68b1554724edebe9a",
            "30db2535f80b4088842def7574ba0681",
            "a6afee10af924486941497991c46a9f8",
            "0d6fa633f9734b63a8eb8cbda44388f3"
          ]
        },
        "id": "rgRIpo95VT92",
        "outputId": "f7f994dc-48c2-4f07-b40f-36bc23b03d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-14 17:41:33--  https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entity2text.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entity2text.txt [following]\n",
            "--2024-04-14 17:41:33--  https://raw.githubusercontent.com/zjunlp/Relphormer/main/dataset/fb15k-237/entity2text.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 392383 (383K) [text/plain]\n",
            "Saving to: ‘entity2text.txt’\n",
            "\n",
            "entity2text.txt     100%[===================>] 383.19K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-04-14 17:41:34 (60.1 MB/s) - ‘entity2text.txt’ saved [392383/392383]\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0b409dd6ab344c6837ac8b6436be433",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/14951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ent2text\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/entity2text.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entity2text.txt\n",
        "path = \"/content/entity2text.txt\"\n",
        "\n",
        "ent2text = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    ent, text = line.strip().split('\\t')\n",
        "    ent2text[ent] = _tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6321346b3cd041b6b3dcd057cbf63cb3"
          ]
        },
        "id": "zwcf9uvWVnvO",
        "outputId": "6a7df34e-68af-4c25-9cd3-bef4da1269d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6321346b3cd041b6b3dcd057cbf63cb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/237 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# rel2text\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/relation2text.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/relation2text.txt\n",
        "path = \"/content/relation2text.txt\"\n",
        "\n",
        "rel2text = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    rel, text = line.strip().split('\\t')\n",
        "    rel2text[rel] = _tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "dd4198022bca4910bc2019a5ca95a3f7"
          ]
        },
        "id": "bKskuy2_V7OE",
        "outputId": "c4018d4c-3c9e-40d3-b726-6dbc5d1f2242"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd4198022bca4910bc2019a5ca95a3f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/14951 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# ent2decs\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/entity2textlong.txt'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/entity2textlong.txt\n",
        "path = \"/content/entity2textlong.txt\"\n",
        "\n",
        "ent2decs = {}\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    ent, text = line.strip().split('\\t')\n",
        "    ent2decs[ent] = _tokenize(text)[:64]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "769b3a95a4dd404cbf86142ae3a54261"
          ]
        },
        "id": "4GIGwYRBbJiY",
        "outputId": "35beaebd-6c55-4ee4-8c3d-ef9221cdc183"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "769b3a95a4dd404cbf86142ae3a54261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/272115 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train triplet\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/train.tsv'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/train.tsv\n",
        "path = \"/content/train.tsv\"\n",
        "\n",
        "train_triplet_id = []\n",
        "train_triplet_tokens = []\n",
        "train_triplet_decs = []\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    head, relation, tail = line.strip().split('\\t')\n",
        "    head_id = ent2id[head]\n",
        "    relation_id = rel2id[relation]\n",
        "    tail_id = ent2id[tail]\n",
        "    head_tokens = ent2text[head]\n",
        "    relation_tokens = rel2text[relation]\n",
        "    tail_tokens = ent2text[tail]\n",
        "    head_decs = ent2decs[head]\n",
        "    tail_decs = ent2decs[tail]\n",
        "    train_triplet_id.append((head_id, relation_id, tail_id))\n",
        "    train_triplet_tokens.append((head_tokens, relation_tokens, tail_tokens))\n",
        "    train_triplet_decs.append((head_decs, tail_decs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a200842eb4034af18aed14dbce82f206"
          ]
        },
        "id": "CweuCo-XZ_4F",
        "outputId": "5fbb0d9b-e40e-4b7a-fde6-2ec7c3374c7f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a200842eb4034af18aed14dbce82f206",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/17535 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# valid triplet\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/dev.tsv'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/dev.tsv\n",
        "path = \"/content/dev.tsv\"\n",
        "\n",
        "valid_triplet_id = []\n",
        "valid_triplet_tokens = []\n",
        "valid_triplet_decs = []\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    head, relation, tail = line.strip().split('\\t')\n",
        "    head_id = ent2id[head]\n",
        "    relation_id = rel2id[relation]\n",
        "    tail_id = ent2id[tail]\n",
        "    head_tokens = ent2text[head]\n",
        "    relation_tokens = rel2text[relation]\n",
        "    tail_tokens = ent2text[tail]\n",
        "    head_decs = ent2decs[head]\n",
        "    tail_decs = ent2decs[tail]\n",
        "    valid_triplet_id.append((head_id, relation_id, tail_id))\n",
        "    valid_triplet_tokens.append((head_tokens, relation_tokens, tail_tokens))\n",
        "    valid_triplet_decs.append((head_decs, tail_decs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "70179d7997b74d3aae6b742c6cea4941"
          ]
        },
        "id": "Ylq6tBF9ctCI",
        "outputId": "6ec4be5b-e3b9-4378-87f3-13a4872dec6c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70179d7997b74d3aae6b742c6cea4941",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing lines:   0%|          | 0/20466 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# test triplet\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/test.tsv'):\n",
        "  !wget https://github.com/zjunlp/Relphormer/raw/main/dataset/fb15k-237/test.tsv\n",
        "path = \"/content/test.tsv\"\n",
        "\n",
        "test_triplet_id = []\n",
        "test_triplet_tokens = []\n",
        "test_triplet_decs = []\n",
        "with open(path, \"r\") as f:\n",
        "  total_lines = sum(1 for _ in f)\n",
        "  f.seek(0)  # Reset file pointer to the beginning\n",
        "  for i, line in tqdm(enumerate(f), total=total_lines, desc=\"Processing lines\"):\n",
        "    head, relation, tail = line.strip().split('\\t')\n",
        "    head_id = ent2id[head]\n",
        "    relation_id = rel2id[relation]\n",
        "    tail_id = ent2id[tail]\n",
        "    head_tokens = ent2text[head]\n",
        "    relation_tokens = rel2text[relation]\n",
        "    tail_tokens = ent2text[tail]\n",
        "    head_decs = ent2decs[head]\n",
        "    tail_decs = ent2decs[tail]\n",
        "    test_triplet_id.append((head_id, relation_id, tail_id))\n",
        "    test_triplet_tokens.append((head_tokens, relation_tokens, tail_tokens))\n",
        "    test_triplet_decs.append((head_decs, tail_decs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "oEbqwTAsxAAn",
        "outputId": "904013e8-2770-45c2-c6dd-b8a480f80cd3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9fe9b06b2e27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KleR6FyTllai"
      },
      "outputs": [],
      "source": [
        "id2ent_rotatE = model.data_indexer.backend.entities_dict\n",
        "RotatE_ent = torch.load(\"/content/RotatE_ent.pt\")\n",
        "\n",
        "new_RotatE_ent_dict = {}\n",
        "for rid, rent in id2ent_rotatE.items():\n",
        "  emb = RotatE_ent[rid]\n",
        "  ent_id = ent2id[rent]\n",
        "  new_RotatE_ent_dict[ent_id] = emb\n",
        "\n",
        "new_RotatE_ent = []\n",
        "for i in range(len(id2ent)):\n",
        "  if i in new_RotatE_ent_dict:\n",
        "    new_RotatE_ent.append(new_RotatE_ent_dict[i])\n",
        "\n",
        "new_RotatE_ent = torch.stack(new_RotatE_ent)\n",
        "\n",
        "id2rel_rotateE = model.data_indexer.backend.relations_dict\n",
        "RotatE_rel = torch.load(\"/content/RotatE_rel.pt\")\n",
        "\n",
        "new_RotatE_rel_dict = {}\n",
        "for rid, rrel in id2rel_rotateE.items():\n",
        "  emb = RotatE_rel[rid]\n",
        "  rel_id = rel2id[rrel]\n",
        "  new_RotatE_rel_dict[rel_id] = emb\n",
        "\n",
        "new_RotatE_rel = []\n",
        "for i in range(len(id2rel)):\n",
        "  if i in new_RotatE_rel_dict:\n",
        "    new_RotatE_rel.append(new_RotatE_rel_dict[i])\n",
        "\n",
        "new_RotatE_rel = torch.stack(new_RotatE_rel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8qSNPR9VN30"
      },
      "outputs": [],
      "source": [
        "kgt5_data = {\n",
        "    \"train_triplet_id\":train_triplet_id,\n",
        "    \"train_triplet_tokens\":train_triplet_tokens,\n",
        "    \"train_triplet_decs\":train_triplet_decs,\n",
        "    \"valid_triplet_id\":valid_triplet_id,\n",
        "    \"valid_triplet_tokens\":valid_triplet_tokens,\n",
        "    \"valid_triplet_decs\":valid_triplet_decs,\n",
        "    \"test_triplet_id\":test_triplet_id,\n",
        "    \"test_triplet_tokens\":test_triplet_tokens,\n",
        "    \"test_triplet_decs\":test_triplet_decs,\n",
        "    \"RotatE_ent_emb\":new_RotatE_ent,\n",
        "    \"RotatE_rel_emb\":new_RotatE_rel,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7dhJMRoryiO",
        "outputId": "657b46be-0fc3-4cf6-c81d-1b7b254ae8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://kgt5_data.pt [Content-Type=application/vnd.snesdev-page-table]...\n",
            "|\n",
            "Operation completed over 1 objects/87.0 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.save(kgt5_data, \"/content/kgt5_data.pt\")\n",
        "!gsutil cp kgt5_data.pt gs://hien7613storage2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YE_B8Kk0-PK",
        "outputId": "ac5e4ac5-8d60-49d0-a660-5836c538bc82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train_triplet_id', 'train_triplet_tokens', 'train_triplet_decs', 'valid_triplet_id', 'valid_triplet_tokens', 'valid_triplet_decs', 'test_triplet_id', 'test_triplet_tokens', 'test_triplet_decs', 'RotatE_ent_emb', 'RotatE_rel_emb'])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kgt5_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "652V4QSU0-X5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06ea347494ca48b68b1554724edebe9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6fa633f9734b63a8eb8cbda44388f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30db2535f80b4088842def7574ba0681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54f2a934e5cf453c8221d684ee8a6e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6afee10af924486941497991c46a9f8",
            "placeholder": "​",
            "style": "IPY_MODEL_0d6fa633f9734b63a8eb8cbda44388f3",
            "value": " 14951/14951 [00:02&lt;00:00, 7399.59it/s]"
          }
        },
        "62a21e3f23164eae9d0a222807ccce84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f5196b8a08748a698b3cb628bada525",
            "placeholder": "​",
            "style": "IPY_MODEL_9920b81630794f33a7fec9862e53d477",
            "value": "Processing lines: 100%"
          }
        },
        "6f82b483b786446a96b7f536ade3fbe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5196b8a08748a698b3cb628bada525": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9920b81630794f33a7fec9862e53d477": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6afee10af924486941497991c46a9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5aadc209da4ef8b21e176826290e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06ea347494ca48b68b1554724edebe9a",
            "max": 14951,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30db2535f80b4088842def7574ba0681",
            "value": 14951
          }
        },
        "f0b409dd6ab344c6837ac8b6436be433": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a21e3f23164eae9d0a222807ccce84",
              "IPY_MODEL_ae5aadc209da4ef8b21e176826290e93",
              "IPY_MODEL_54f2a934e5cf453c8221d684ee8a6e5a"
            ],
            "layout": "IPY_MODEL_6f82b483b786446a96b7f536ade3fbe9"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}